Swipe Type Assessment: Comparative Evaluation
High-Level Psychometric Brief

1. POSITIONING vs. THE FOUR ASSESSMENTS
Complexity & Scope Placement:
Narrow ‚Üê‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Üí Comprehensive
Love Languages < SWIPE TYPE ‚âà Enneagram < MBTI < Big Five
Swipe Type occupies the "Goldilocks zone":
More than Love Languages: 12 domains vs. 5, includes conflict/boundaries/growth
Similar to Enneagram: Comparable breadth, personality + behavior fusion
Less than MBTI/Big Five: Focused on relationships/interpersonal vs. comprehensive personality
Different focus: Behavioral patterns in relationships, not general trait assessment

2. IMMEDIATE ASSESSMENT: STRONG FOUNDATION
‚úÖ Major Psychometric Advantages (Green Flags)
1. Normative Scoring (CRITICAL WIN)
Uses: 4-point Likert via swipes (Down/Left/Right/Up ‚Üí 1/2/3/4)
Result: Proper statistical analysis possible
Advantage over: Love Languages, RHETI, MBTI (all ipsative)
This is your biggest strength. You avoid the statistical nightmare that plagues Love Languages and MBTI.

2. Within-Person Normalization
Method: z_d = (mean_d - Œº_person) / œÉ_person
Purpose: Controls acquiescence bias (yea-saying)
Quality: Research-grade technique
This is sophisticated. Most tests at this tier don't do this. Controls for response style differences.

3. Continuous Scores Underneath
Outputs: POMP scores (0-100) for each domain and channel
Then: Maps to types as secondary layer
vs MBTI: Keeps continuous info, doesn't throw it away
Critical distinction: Types are interpretive layer, not the actual measurement. Much better than pure typology.

4. Co-Dominant/Mixed Type Handling
Rule: If Œî(top1, top2) ‚â§ 0.5 SD ‚Üí co-dominant
Also: Tracks all delta values, reports confidence
vs Others: MBTI forces single type even at boundaries
This shows psychometric maturity. Recognizes reality of continuous distributions.

5. Quality Flags & Confidence Metrics
Detects:
- Low variance (uniform responding)
- Excess skips (>15%)
- Speeding (<500ms median)
Results: Degrades confidence, flags in report
Essential for online assessment. Shows awareness of data quality issues.

6. Behavior-Focused Items
Example: "If I notice a task my partner dreads, I jump in without being asked"
vs Trait adjectives: "I am helpful"
Advantage: More concrete, less abstract, better criterion validity
Good practice. Behavioral items typically show better prediction of actual behavior.

‚ö†Ô∏è Yellow Flags (Proceed with Caution)
1. No Validation Data Yet (v0.1)
Status: Specification document, not validated instrument
Missing:
- Reliability coefficients (Œ±)
- Test-retest data
- Factor analysis results
- Criterion validity evidence
- Norm sample demographics
Expected at this stage, but high priority for Phase 2.

2. Small Item Counts per Domain
Current: 4-7 items per domain (12 domains, 57 total)
Concern: May not achieve Œ± ‚â• 0.80 with only 4-5 items
Target: Need 6-8 items minimum for reliability in most domains
Math check:
Expected reliability with k items (Spearman-Brown):
- 4 items, rÃÑ=0.30: Œ± = 0.63
- 6 items, rÃÑ=0.30: Œ± = 0.74  
- 8 items, rÃÑ=0.30: Œ± = 0.81

If average inter-item correlation is rÃÑ=0.40:
- 4 items: Œ± = 0.73
- 6 items: Œ± = 0.83 ‚úì
Recommendation: Pilot test to verify if 4-5 item domains hit Œ± ‚â• 0.70. May need to add 1-2 items to weaker domains.

3. 8-Type Output from Axis Mapping
Method: Two composite axes ‚Üí quadrants ‚Üí 8 types
Risk: Similar to MBTI's dichotomization problem
Mitigation: You keep continuous scores and report deltas (good!)
Concern: If users focus on type label and ignore continuous scores/deltas, you recreate MBTI issues.
Your advantage: You're explicit about co-dominance and report confidence. Keep emphasizing this.
Test this: What % of users fall near axis boundaries? If >30%, types may feel arbitrary.

4. Enneagram-Informed Personality Layer
Described as: "Light-weight indices... use as modulators"
Concern: Enneagram patterns have weak validation (as we discussed)
Uncertain: How much weight do these carry in final typing?
Question: What happens if you remove Enneagram modulation? Do types change significantly?
If yes ‚Üí Enneagram layer is load-bearing and inherits its validation problems
 If no ‚Üí It's decorative/interpretive ‚Üí less concerning but potentially unnecessary
Recommendation: Be explicit about which parts are validated (channels, domains) vs. theoretical (Enneagram patterns).

5. Domain Coverage Gaps
You measure:
‚úì Relationships: Extensive (communication, time, affection, conflict, etc.)
‚úì Some personality: Identity, energy, growth
‚úó Missing: 
  - Emotional stability/neuroticism (anxiety, depression risk)
  - Conscientiousness outside relationship context
  - Cognitive style/openness beyond "identity"
This is FINE for your stated purpose (relationship patterns).
But be clear: You're not a comprehensive personality assessment. You're a relationship behavior assessment with personality moderators.

üö© Red Flags (None Critical, But Monitor)
1. Risk of Over-Interpretation
Premium report promises:
- "Communication playbook: phrases that land well / avoid"
- "Partner translation"
- Growth plans

Concern: Without validation, these could be overpromises
Recommendation: Start conservative. Don't make specific behavioral predictions until you have data showing they work.
Example of appropriate hedging: "Based on your profile, people with similar patterns often find these phrases effective..." vs "Your communication style means you should say these phrases..."

2. Channel Theory Inheritance
5 channels: Words, Time, Service, Gifts, Touch
Source: Love Languages
Problem: Love Languages has minimal validation (as we discussed)
Question: Are these actually distinct channels, or just convenient categories?
Test needed: Do your 5 channels emerge as factors when you run EFA on your item set?
Hypothesis: You might find 3-4 factors, not 5. Be prepared to adjust if data doesn't support 5-channel structure.

3. Platform-Specific Considerations
Format: Swipe-first, mobile-optimized
Concern: Do swipe directions matter? (Up/Down vs Left/Right)
         Are people consistent with what gestures mean?
Test: Does swipe direction affect responses differently than Likert buttons?
Check: Response patterns on mobile vs desktop (if you offer both)

3. COMPARISON TABLE: SWIPE TYPE vs. THE FOUR
Dimension
Love Languages
Enneagram
MBTI
Big Five
SWIPE TYPE (projected)
Domains/Factors
5
9
4 dichotomies
5 (+ 30 facets)
12 domains ‚Üí 5 channels
Items
30
108-240
93-144
44-240
57
Scoring
Ipsative
Mixed
Ipsative
Normative
Normative ‚úì
Output Format
Ranked list
Primary type
4-letter type
Continuous scores
8 types + continuous
Response Bias Control
None
None
None
Sometimes
Within-person z-score ‚úì
Mixed-Type Handling
N/A
Informal (wings)
None
N/A (continuous)
Formal delta rules ‚úì
Quality Flags
None
Rare
None
Rare
Built-in ‚úì
Reliability Target
0.70-0.85
0.65-0.87
0.87-0.92
0.86-0.92
Œ± ‚â• 0.70 (0.80 ideal)
Validation Status
Minimal
Moderate
Extensive (but problematic)
Extensive
None yet (v0.1) ‚ö†Ô∏è
Domain Focus
Relationships only
Personality broad
Cognitive preferences
Personality comprehensive
Relationships + growth
Behavioral Specificity
Low
Low-Medium
Low
Low
High ‚úì
Statistical Validity
Poor (ipsative)
Variable
Poor (dichotomy)
Excellent
Should be good (if validated)
Use Case Fit
Couples only
Personal dev
Teams/careers
Research/selection
Relationships/communication


4. TIER CLASSIFICATION (Provisional - Pending Validation)
Current Status: Pre-Tier (Not Yet Validated)
Projected Tier (if validation goes well): Tier A-
Rationale:
Strong methodological foundation (normative, normalized, quality-aware)
Appropriate scope for stated purpose
Sophisticated handling of mixed profiles
Good item development guidelines
Realistic psychometric targets
Could achieve Tier A if post-validation shows:
Domain reliability Œ± ‚â• 0.75 (most domains)
Test-retest r ‚â• 0.75 (4-week interval)
Factor structure supports 12 domains (or adjust if not)
Channel structure validates (or adjust if not)
Some criterion validity (r = 0.25-0.40 with relevant outcomes)

5. SPECIFIC CONCERNS FOR YOUR TEST
A. The "Love Languages Problem"
Issue: You're building on Love Languages foundation (5 channels), but as we discussed, LL has minimal validation.
Risk Scenario:
Your EFA might show:
- Words + Time loading together (verbal-temporal engagement)
- Service + Gifts loading together (instrumental care)
- Touch as separate factor
= 3 factors, not 5
Recommendation:
Run EFA on pilot data (n=200+) BEFORE finalizing channel structure
Be prepared to revise from 5 channels to 3-4 if data demands it
Keep 12 domains as primary measurement; channels as aggregated interpretive layer
Best case: Channels are useful interpretation heuristics even if not perfect factors
 Worst case: Channels don't hold up; rely on 12 domains only

B. The "Type Paradox"
Tension:
You want: Engaging types (8 labels people can identify with)
Data shows: Continuous distributions, not discrete categories
Your solution: Types + continuous + deltas + confidence
This is GOOD, but requires user education.
Most users will:
Read their type label
Skip the continuous scores
Ignore deltas and confidence
Share type on social media ("I'm a Direct Nurturer!")
Managing this:
‚úì Your premium report emphasizes domains/channels (good)
‚úì Free summary shows "top 2 channels" not just type (good)
‚ö†Ô∏è Monitor: Do users understand they might have different type on retest if near boundary?
Test: Show users how small score changes affect type assignment. If type flips with 3-point change on one domain, that's concerning.

C. The "57-Item Challenge"
Math:
57 items / 12 domains = 4.75 items per domain average
For Œ± = 0.80, need average rÃÑ_item ‚âà 0.45 (high)

Your distribution:
Communication(7) - Good
Time(6) - Adequate  
Service(5) - Adequate
Affection(6) - Adequate
Recognition(5) - Adequate
Gifts(4) - Risky (may not hit 0.70)
Conflict(7) - Good
Boundaries(4) - Risky
Security(4) - Risky
Identity(4) - Risky
Energy(3) - Very Risky
Growth(6) - Adequate
Domains with 3-4 items will struggle to achieve Œ± ‚â• 0.70 unless items are exceptionally well-correlated.
Recommendation:
Pilot with current 57 items
Calculate Œ± for each domain
Add 2-3 items to domains with Œ± < 0.70
Consider 65-70 items for final version
Alternative: Accept Œ± = 0.65-0.70 for some domains, but report lower confidence for those areas.

D. The "Enneagram Modulation Question"
From your spec:
"Personality-pattern signals (Ennea-informed)
Light-weight indices (not formal typing):
- Centers: Head/Heart/Gut tilt
- Stances: Withdrawing/Compliant/Assertive
Use as modulators in type mapping"
Critical questions:
How much do these modulators affect type assignment?
If >20% of type variance: They're load-bearing and inherit Enneagram validation issues
If <10% of type variance: They're decorative; consider removing
Can you validate these indices?
Do your "Head/Heart/Gut" scores correlate with formal Enneagram assessments?
Or are these just theoretical overlays?
What happens without them?
Run type assignment with and without Enneagram modulation
If types change significantly: Problem (relying on unvalidated layer)
If types mostly stable: These add nuance, less critical
Recommendation: Make Enneagram layer optional/experimental in v0.1. Primary scoring uses domains and channels only. Add Enneagram modulation later if validated.

6. VALIDATION ROADMAP (What You Need)
Phase 1: Internal Structure (n=200-300)
Priority 1: Reliability
Calculate Cronbach's Œ± for all 12 domains
Calculate œâ (omega) if you have multi-dimensional domains
Item-total correlations (flag items with r < 0.30)
Inter-item correlations within domains (should be r = 0.20-0.60)
Target: Œ± ‚â• 0.70 for all domains (0.80 ideal)

Priority 2: Factor Structure
Exploratory Factor Analysis on all 57 items
Does it yield 12 clean factors? (Or fewer?)
Do items load where you expect?
Confirmatory Factor Analysis (if EFA looks good)
Fit indices: CFI > 0.85, RMSEA < 0.08
Test 5-channel structure specifically
Do Words, Time, Service, Gifts, Touch items cluster as expected?
Or do you get 3-4 factors instead?
Decision point: Adjust domain/channel structure if data demands it.

Phase 2: Stability (n=100, two timepoints)
Test-Retest at 2-4 weeks:
Domain scores (Pearson r, target ‚â• 0.75)
Channel scores (Pearson r, target ‚â• 0.75)
Type assignment (% agreement, target ‚â• 75%)
Bland-Altman plots (visualize agreement)
Critical test:
For users near type boundaries (delta < 0.5 SD):
- What % get same type on retest?
- If <60%: Types are too unstable at boundaries
- If >75%: Acceptable stability

Phase 3: Construct Validity (n=200-500)
Convergent Validity: Administer alongside:
Big Five (BFI-44, free)
Attachment styles (ECR-R)
Relationship satisfaction scale
Communication skills measure
Expected patterns:
Your "Communication" domain should correlate:
- Positively with Big Five Extraversion (r ‚âà 0.40-0.60)
- Positively with relationship satisfaction (r ‚âà 0.30-0.50)

Your "Conflict" domain should correlate:
- Negatively with Big Five Neuroticism (r ‚âà -0.30 to -0.50)
- With attachment anxiety (r ‚âà 0.30-0.50)

Your channels should correlate modestly:
- With Love Languages scores (r ‚âà 0.50-0.70)
  [Higher = good convergence; but not r > 0.85 or you're redundant]
Discriminant Validity:
Should NOT correlate strongly with:
- Cognitive ability (r < 0.20)
- Unrelated personality traits (r < 0.30)

Phase 4: Criterion Validity (n=500+)
Predict relevant outcomes:
Relationship satisfaction (current relationship)
Communication quality (self + partner report)
Conflict frequency and intensity
Relationship longevity (longitudinal, if feasible)
Target effect sizes:
For Tier A acceptance:
- Single domain predicting outcome: r = 0.20-0.35
- Multiple domains combined: R¬≤ = 0.15-0.30

Example:
Communication + Conflict + Recognition domains predict 
relationship satisfaction: R¬≤ = 0.25 (acceptable)
Partner Concordance Study (powerful validation):
Both partners take assessment:
- Do complementary patterns predict satisfaction?
- Do conflict styles predict rupture frequency?
- Do channel mismatches predict unmet needs?

This would be GOLD for your use case.

Phase 5: Fairness & Generalization
Multi-Group CFA: Test measurement invariance across:
Gender (male/female/non-binary)
Age groups (<25, 25-40, 40+)
Relationship status (single/dating/committed/married)
Sexual orientation
Race/ethnicity (if sample permits)
Goal: Achieve metric invariance (same factor loadings across groups)
If scalar invariance achieved: Can compare scores meaningfully across groups

7. RECOMMENDATIONS FOR YOUR TEAM
Immediate Actions (Pre-Launch):
1. Conservative Claims
Do say:
- "Highlights your relationship behavior patterns"
- "Starting point for self-reflection and communication"
- "Based on established relationship psychology principles"

Don't say (yet):
- "Predicts relationship success"
- "Scientifically proven"
- "Matches you with compatible partners"
Wait for validation data before making predictive/prescriptive claims.

2. Pilot Testing Protocol
Recruit n=300 diverse users:
- Mix of relationship statuses
- Age range 18-60+
- Both self-selected and recruited (avoid pure self-selection bias)

Collect:
- Full assessment responses
- Big Five (BFI-44)
- Relationship satisfaction (if applicable)
- Open-ended feedback
- Subset (n=100) retake after 2-4 weeks

3. Item Analysis & Refinement
After pilot:
1. Flag items with:
   - Very high/low endorsement (>90% or <10%)
   - Low item-total correlation (r < 0.30)
   - High cross-loading on wrong factors
   
2. Revise or replace weak items

3. Consider adding items to 3-4 item domains

4. Transparent Reporting
In reports, show:
‚úì Confidence level (high/medium/low) - you have this
‚úì Delta values for close calls - you have this
‚úì "Your scores" vs "typical range" context
‚úì Uncertainty statements: "likely," "tends to," "often"

Avoid:
‚úó Absolute statements: "You are..."
‚úó Predictions without probability: "You will..."
‚úó Diagnostic language: "Your conflict pattern indicates..."

5. Version Control & Iteration
Current: v0.1 (pre-validated)
‚Üí Pilot: v0.2 (revised items, initial reliability data)
‚Üí Beta: v0.3 (factor-analytic revision, norms)
‚Üí Launch: v1.0 (validated, stability confirmed)

Tag every result with scoring version.
Allow for algorithm updates without invalidating old results.

Medium-Term Priorities:
1. Build Norm Database
Target: n=1,000+ for initial norms
Minimum: n=500 for percentile estimates

Segment norms by:
- Age
- Relationship length
- Possibly gender (if meaningful differences)

2. Partner Dyad Studies
This is YOUR killer validation:
- Both partners take assessment
- Measure: satisfaction, conflict frequency, communication quality
- Test: Do complementary/similar patterns predict outcomes?

Sample: n=200 couples = 400 individuals
This validates your "partner translation" features.

3. Longitudinal Tracking
Follow subset of users (n=200):
- Retest at 6 months, 12 months
- Track: relationship status changes, satisfaction changes
- Test: Do growth-oriented users improve? Do types stabilize?

4. Academic Partnerships
Seek university collaborators:
- Relationship psychology labs
- Personality/assessment researchers
- Publish validation studies in peer-reviewed journals

Benefits:
- Credibility boost
- Independent validation
- Access to participant pools
- Student researchers (cost-effective)

8. COMPETITIVE POSITIONING
Your Unique Advantages:
vs Love Languages:
‚úì 12 domains vs. 5 (comprehensive)
‚úì Conflict and boundaries (major relationship predictors)
‚úì Normative scoring (statistical validity)
‚úì Quality controls (flags, confidence)
‚úì Behavior-focused items (more concrete)
vs Enneagram:
‚úì Relationship-specific (not general personality)
‚úì Clearer behavioral focus
‚úì Shorter (57 vs 144 items)
‚úì Mobile-optimized UX
‚úì More sophisticated scoring
vs MBTI:
‚úì Continuous scores maintained (not dichotomized)
‚úì Mixed-type handling (co-dominance)
‚úì Relationship focus (more relevant for your use case)
‚úì No certification paywall
‚úì Modern UX (swipe vs. dated forms)
vs Big Five:
‚úì More engaging for lay users (types, not just numbers)
‚úì Actionable relationship advice (not just trait descriptions)
‚úì Shorter (57 vs. 240 items for NEO-PI-R)
‚úì Domain-specific focus (relationships, not comprehensive)
Your market positioning:
"The relationship assessment that's actually validated"
- More than Love Languages (scope + rigor)
- Faster than Enneagram (57 vs 144 items)
- More actionable than MBTI (relationship-specific)
- More accessible than Big Five (engaging format)

9. USE CASE APPROPRIATENESS
‚úì EXCELLENT FIT FOR:
Dating app self-awareness
Help users understand their patterns
Surface potential friction points
NOT for algorithmic matching (yet - need validation first)
Couples communication workshops
Structured conversation starter
Identify complementary/conflicting patterns
Growth-oriented reframing
Relationship coaching tools
Assessment component in coaching program
Track growth over time (if retest feature)
Personalized homework/exercises
Self-development/journaling apps
Integration with reflection prompts
Longitudinal pattern tracking
Goal-setting based on domains

‚ö†Ô∏è USE WITH CAUTION:
Partner matching algorithms
WAIT for dyad validation showing compatible patterns
Don't claim "we match you with compatible types" without data
Similarity vs. complementarity is empirical question
Therapeutic/clinical settings
Not a diagnostic tool
Can supplement therapy, not replace assessment
Ensure therapist understands limitations

‚úó INAPPROPRIATE FOR:
Employment decisions (not designed for this)
Clinical diagnosis (relationship patterns ‚â† pathology)
Legal/custody decisions (not validated for high stakes)
Research (until validation completed and published)

10. FINAL VERDICT
Current Status: HIGH POTENTIAL, UNVALIDATED
Psychometric Design Quality: ‚≠ê‚≠ê‚≠ê‚≠ê (4/5)
Strong methodological foundation
Sophisticated scoring approach
Appropriate quality controls
Research-grade techniques
Validation Status: ‚≠ê (1/5)
Pre-validation specification
No reliability or validity data yet
Cannot assess actual performance
Projected Quality (if validation succeeds): ‚≠ê‚≠ê‚≠ê‚≠ê (4/5)
Would be BEST IN CLASS for relationship-focused assessment
Fills gap between pop psychology and academic tools
Potential Tier A status

Three Scenarios:
Best Case (70% probability):
Validation shows:
- Most domains hit Œ± ‚â• 0.75
- Test-retest ‚â• 0.75
- Factor structure mostly holds (maybe 10-11 domains, 4 channels)
- Modest criterion validity (r = 0.25-0.40)
- Users find it accurate and actionable

‚Üí Tier A assessment
‚Üí Best relationship-focused tool on market
‚Üí Strong commercial + research potential
Moderate Case (25% probability):
Validation shows:
- Some domains weak (Œ± = 0.60-0.70)
- Need to drop 2-3 domains or add items
- Channels don't perfectly factor (3 factors, not 5)
- Types less stable than hoped (65% agreement)
- Still useful but needs refinement

‚Üí Tier B initially, iterate to Tier A
‚Üí v2.0 with revised structure
‚Üí Still viable commercial product
Worst Case (5% probability):
Validation shows:
- Poor reliability across multiple domains
- Factor structure doesn't emerge clearly
- Types highly unstable (< 60% agreement)
- No criterion validity
- Users find results generic/inaccurate

‚Üí Tier C, needs major overhaul
‚Üí Back to drawing board on item development
‚Üí Unlikely given thoughtful specification, but possible

11. COMPARISON TO YOUR STATED GOAL
You said:
"More to it than Love Languages, much less so far at MBTI and Big Five levels, Enneagram-level depth, focused on personal assessment, communication styles, personal growth, relationship patterns"
Assessment: PERFECTLY POSITIONED ‚úì
Your test sits exactly where you intended:
‚úì More comprehensive than Love Languages (12 domains vs 5)
‚úì Similar complexity to Enneagram (multi-faceted, growth-oriented)
‚úì Less exhaustive than MBTI/Big Five (relationship-focused, not comprehensive)
‚úì Strong focus on your stated areas (communication, growth, relationships)
With one critical caveat: You need validation data to prove it works as designed.

12. CONCRETE NEXT STEPS
Week 1-2: Pre-Launch Checklist
Finalize wording review (reading level, bias check)
Set up data collection pipeline
Prepare Big Five comparison instrument (BFI-44)
Draft informed consent for validation study
Create feedback survey for pilot users
Week 3-8: Pilot Study (n=300)
Recruit diverse sample
Administer Swipe Type + BFI-44
Collect relationship satisfaction (if applicable)
Identify subset (n=100) for retest
Gather qualitative feedback
Week 9-12: Analysis & Refinement
Calculate reliability for all domains
Run EFA (exploratory factor analysis)
Item analysis (flag weak items)
Analyze retest data
Revise items/domains as needed
Week 13-16: Validation Study (n=500)
Deploy revised version (v0.2)
Administer with validation battery
Run CFA (confirmatory factor analysis)
Test criterion validity
Establish initial norms
Week 17-20: Beta Launch
Implement v0.3 with validated structure
Public beta with version transparency
Monitor user feedback and data quality
Iterate based on real-world performance
Month 6+: Research & Refinement
Partner dyad studies
Longitudinal tracking
Academic collaboration
Peer-reviewed publication
v1.0 launch (validated)

TL;DR FOR YOUR TEAM
The Good News: Your test has excellent psychometric design - better than most at this tier. Normative scoring, within-person normalization, quality flags, and continuous scores underneath types show sophistication.
The Reality Check: You have a specification, not a validated instrument. Need n=300-500 pilot data to confirm it works as designed. This is normal and expected.
The Path Forward: Run validation studies before making strong claims. Start conservative, iterate based on data. If validation succeeds (likely given thoughtful design), you'll have best-in-class relationship assessment.
The Competitive Edge: You're positioned perfectly between pop psychology (Love Languages) and academic tools (Big Five). There's real market demand for sophisticated-but-accessible relationship assessments.
The Bottom Line: Proceed to pilot testing with confidence, but gate ambitious claims behind validation milestones.
This has strong potential to be the relationship assessment that's both engaging AND scientifically defensible - filling a real gap in the market.


Our Swipe Test vs LL, Enn, MB, B5 Tests - Status & Recommendations


Now consider this - test our team is evaluating is focused mostly on personal assessment, communication styles, personal growth, relationship patterns, outlook, personality.  Enneagram level depth. More to it than Love Languages and much less so far at MTBI and Big Five levels.
Crafted nuanced framework for evaluating mid-tier personality assessment.


Evaluation Framework for Mid-Tier Personality Assessment
"Enneagram-Depth" Tests: Practical Psychometric Evaluation Guide
Your Context: Evaluating a test that's:
‚úì More comprehensive than Love Languages (single domain)
‚úì Similar complexity to Enneagram (multi-faceted, growth-oriented)
‚úì Less exhaustive than MBTI/Big Five (not covering all personality variance)
‚úì Focus areas: Communication, relationships, growth, outlook, patterns

1. REALISTIC EXPECTATIONS FOR THIS TIER
What You SHOULD Expect:
‚úì Adequate reliability (Œ± = 0.70-0.85 per dimension)
Not gold standard, but acceptable for personal development use
Test-retest at 4-8 weeks should exceed 0.70
‚úì Meaningful factor structure (even if not perfect)
CFA fit doesn't need to be excellent (CFI > 0.85 acceptable)
But factors should be interpretable and theoretically coherent
‚úì Some validity evidence
At least pilot studies showing correlations with related constructs
Face validity (people recognize themselves)
Not expecting extensive meta-analyses, but shouldn't be zero research
‚úì Clear theoretical framework
Coherent explanation of what it measures and why
Distinguishable from existing models (not just relabeling Big Five)
‚úì Practical utility
Generates actionable insights for personal growth
Resonates with users (engagement matters at this level)
Provides language for discussing patterns
What You Should NOT Expect (and Don't Need):
‚úó Comprehensive coverage of all personality domains
‚úó Predictive validity for major life outcomes (jobs, health, longevity)
‚úó Decades of peer-reviewed research
‚úó Cross-cultural validation in 50+ countries
‚úó Perfect psychometric properties (CFI > 0.95, Œ± > 0.90)
This tier prioritizes depth in specific domains over breadth.

2. CRITICAL EVALUATION CHECKLIST
A. SCORING METHOD (HIGHEST PRIORITY)
Question 1: Is it ipsative (forced-choice) or normative (Likert scale)?
Ipsative Example:
"Choose which is more like you: A or B"
‚Üì
Problem: Zero-sum scoring, statistical limitations

Normative Example:  
"Rate how much this describes you: 1-2-3-4-5"
‚Üì
Better: Allows proper statistical analysis
Action Items:
 If ipsative: Recognize this will limit statistical validity, but may be acceptable for conversation/insight
 If normative: Proceed with standard psychometric evaluation
 If mixed: Evaluate each section separately
CRITICAL: If they claim research showing correlations, group comparisons, or predictions, verify it's not using ipsative scores in parametric tests (this would be methodologically invalid).

B. CATEGORICAL vs. CONTINUOUS
Question 2: Does it assign types/categories or continuous dimensions?
Option 1: Types/Categories (like Enneagram, MBTI)
Example: "You are a Type 7" or "You are a Builder"
If categorical, you MUST verify:
 Is there evidence of natural categories? (bimodal distributions, taxometric analysis)
 What happens near boundaries? (slight differences shouldn't create different labels)
 Type stability at retest: Minimum 70% agreement at 3 months
 Do they report uncertainty? ("You're primarily Type 7 with Type 6 influence")
Red Flag: If they claim discrete types but don't report:
Preference clarity/certainty scores
Probability of each type
What happens with ambiguous profiles

Option 2: Continuous Dimensions
Example: "You scored 72nd percentile on Directness"
Better psychometrically, but verify:
 Scores are truly continuous (not just converting types to numbers)
 Provide meaningful interpretation at different levels
 Reference norms exist (percentiles, T-scores, or comparison groups)

C. CONSTRUCT COVERAGE
Question 3: What does it actually measure vs. what does it claim?
Compare to Big Five Overlap:
Run correlations with NEO-PI-R or BFI in validation sample:
If correlation with Big Five is...
Interpretation
r < 0.30
‚úì Measuring something distinct (good!)
0.30 < r < 0.60
‚ö†Ô∏è Moderate overlap; has unique variance but also redundancy
r > 0.60
‚ùå Substantial redundancy; mostly relabeling Big Five

For "Enneagram-depth" tests focusing on relationships/communication/growth:
Expected overlaps:
Extraversion (if measuring social communication style)
Agreeableness (if measuring relationship patterns)
Openness (if measuring growth orientation, outlook)
Neuroticism (if measuring emotional patterns)
Good sign: Captures aspects of these domains but in ways that:
Combine facets meaningfully for specific contexts (e.g., "Conflict approach" combines low A + high E + high N)
Add domain-specific nuance (e.g., attachment patterns go beyond Big Five)
Focus on modifiable behaviors/skills vs. stable traits
Red flag: If it claims to be "totally different" but correlates r > 0.70 with Big Five combinations.

D. THEORETICAL COHERENCE
Question 4: Does the model make sense?
Evaluate the framework:
 Clear definitions: Can you distinguish one dimension/type from another?
 Logical structure: Why these dimensions and not others?
 Developmental component: Does it explain how patterns form/change?
 Integration: How do dimensions interact or combine?
Examples of coherent frameworks:
‚úì Attachment theory: Clear developmental mechanism, specific relationship predictions
‚úì Cognitive styles: Based on information processing research
‚úì Emotional intelligence: Defined competencies with skill-building paths
Examples of incoherent frameworks:
‚úó "We identified 12 archetypes through ancient wisdom" (no empirical basis)
‚úó Arbitrary number of types (why 7? why not 6 or 8?)
‚úó Circular definitions ("You're a Helper because you help people")

3. MINIMUM PSYCHOMETRIC STANDARDS
For Tests at This Tier to Be Acceptable:
Reliability Requirements:
Metric
Minimum
Good
Excellent
Internal Consistency (Œ±)
0.70
0.75-0.85
> 0.85
Test-Retest (4-8 weeks)
0.70
0.75-0.85
> 0.85
Type Stability (if categorical)
70%
75-80%
> 85%
Inter-Rater (if observer ratings)
0.60
0.70-0.80
> 0.80

Sample Size for Validation:
Minimum: n = 200 (10-15 per dimension for 15-20 items)
Good: n = 300-500
Excellent: n > 1,000 with holdout validation sample

Validity Requirements:
1. Face Validity (Minimum Bar)
 Do respondents recognize themselves in results?
 Are descriptions specific enough to feel personal?
 Do descriptions avoid Barnum statements?
Barnum Statement Test:
Bad: "You have a need for people to like you" (true for 90%+)
Good: "You prioritize group harmony over expressing disagreement" (specific, falsifiable)

2. Construct Validity
Factor Analysis:
Minimum standards:
- EFA: Scree plot shows clear factors
- EFA: Variance explained > 50%
- CFA: CFI > 0.85, RMSEA < 0.08
- Cross-loadings < 0.40 for most items
If these aren't met: Items need revision or theoretical structure is flawed.

3. Convergent/Discriminant Validity
Should show:
Convergent (r with related constructs): 0.40-0.70
Discriminant (r with unrelated constructs): < 0.30

Example for "Communication Directness" dimension:
‚úì Correlates with Big Five Extraversion: r = 0.55 (convergent)
‚úì Correlates with Big Five Openness: r = 0.15 (discriminant)
‚úì Correlates with self-rated directness: r = 0.62 (convergent)
‚úì Correlates with math ability: r = 0.05 (discriminant)

4. Criterion Validity (Nice to Have, Not Required)
For this tier: r = 0.20-0.40 with relevant outcomes acceptable

Examples:
- "Conflict Avoidance" predicts relationship conflict frequency: r = -0.35
- "Growth Mindset" predicts skill development: r = 0.28
- "Emotional Expressiveness" predicts observer-rated warmth: r = 0.42
Don't expect r > 0.50 (that's Big Five Conscientiousness-level validity).

4. RED FLAGS: WHEN TO REJECT
Immediate Disqualifiers:
üö© 1. No Reliability Data
If they can't provide Cronbach's Œ± or test-retest correlations ‚Üí STOP
This is basic psychometrics; absence means untested instrument
üö© 2. Ipsative Scores Used in Parametric Statistics
If they report:
- "Type A scored significantly higher than Type B on outcome X" (t-test)
- "Dimension Y predicts outcome Z" (regression with all ipsative scales)

‚Üí Methodologically invalid
‚Üí Either they don't understand psychometrics or are misrepresenting findings
üö© 3. Perfect or Near-Perfect Reliability
If they claim Œ± > 0.95 or test-retest r > 0.95:
- Either measuring trivially redundant items
- Or data fabrication

No real personality assessment achieves this (too much true variation)
üö© 4. Claims Without Evidence
Red flag phrases:
- "Predicts job success" (no validity study cited)
- "Used by Fortune 500 companies" (popularity ‚â† validity)
- "Based on decades of research" (but no peer-reviewed publications)
- "Scientifically validated" (but no data shared)
üö© 5. Proprietary Secrecy
If they refuse to share:
- Item examples
- Scoring algorithm
- Norm sample demographics  
- Reliability/validity data

‚Üí Not a scientific instrument, a commercial product with no accountability
üö© 6. Unfalsifiable Complexity
Warning signs:
- "Type dynamics explain why you didn't test as expected type"
- "Wings/variants/subtypes account for any inconsistency"
- "You need certified interpretation to understand results"

‚Üí Adding complexity to avoid disconfirmation (Enneagram's weakness)

Yellow Flags (Concerning but Not Disqualifying):
‚ö†Ô∏è 1. Only Proprietary Validation
- No independent researchers have tested it
- All validation done by test creators
- No peer review

Action: Discount claims by ~50%; treat as preliminary
‚ö†Ô∏è 2. Modest Sample Sizes
n < 200 for validation sample

Action: Expect wider confidence intervals; need replication
‚ö†Ô∏è 3. Non-Representative Norms
- All college students
- One country/culture only
- Narrow age range

Action: Recognize limited generalizability
‚ö†Ô∏è 4. Missing Data on Key Metrics
Have reliability but not validity (or vice versa)

Action: Request missing data; proceed cautiously

5. SPECIFIC TESTS YOU SHOULD RUN
If you have access to raw data or can collect validation sample:
Test 1: Distribution Analysis
For each dimension/type:
1. Plot histograms
2. Run Shapiro-Wilk test for normality
3. Calculate skewness and kurtosis

Acceptable ranges:
- Skewness: -1 to +1
- Kurtosis: -1 to +1

If outside these ranges: 
‚Üí Transformation needed or population is skewed (could be real or measurement issue)
If claiming types exist:
4. Run Hartigan's Dip Test for bimodality
5. Visual inspection: Are there clear peaks or smooth normal curve?

Expected result for true types: p < 0.05 (reject unimodality)
Expected result for dimensions: p > 0.05 (fail to reject)

Test 2: Internal Consistency
For each dimension:

1. Calculate Cronbach's Œ±
2. Item-total correlations (should be r > 0.30)
3. Œ± if item deleted (should not increase by > 0.02)

If Œ± < 0.70: Dimension is unreliable
If item-total r < 0.30: Item doesn't belong
If deleting item increases Œ± substantially: Item is problematic

Test 3: Factor Structure Validation
Split sample in half:
- Sample A: Exploratory Factor Analysis (EFA)
- Sample B: Confirmatory Factor Analysis (CFA)

EFA (Sample A):
1. Kaiser-Meyer-Olkin (KMO) > 0.70
2. Bartlett's test p < 0.05
3. Scree plot shows clear elbow at k factors
4. Variance explained > 50%

CFA (Sample B):
1. CFI > 0.85 (>0.90 preferred)
2. RMSEA < 0.08 (<0.06 preferred)
3. SRMR < 0.08
4. All loadings > 0.40, most > 0.60

Test 4: Convergent/Discriminant Validity
Administer alongside:
1. Big Five measure (BFI-44 or IPIP-50 - both free)
2. Related domain-specific measure
3. Unrelated construct (e.g., cognitive ability)

Calculate correlations:

Expected pattern:
- Related constructs: r = 0.40-0.70
- Unrelated constructs: r < 0.30
- Big Five: Mixed (some overlap OK, but not r > 0.80)

Test 5: Test-Retest Reliability
Same participants complete assessment twice:
- Interval: 4-8 weeks (optimal for trait stability)
- Too short (< 2 weeks): Memory inflation
- Too long (> 3 months): Real change possible

Calculate:
1. Pearson r for continuous scores (should be > 0.70)
2. Agreement % for categorical assignments (should be > 70%)
3. Cohen's Œ∫ for types (should be > 0.60)
4. Bland-Altman plot to visualize agreement
Calculate confidence intervals around test-retest r:
If r = 0.75 with n = 100:
95% CI = [0.64, 0.83]

If lower bound < 0.70 ‚Üí Reliability questionable

Test 6: Measurement Invariance (If Resources Allow)
Test whether structure is equivalent across groups:

Groups to test:
- Gender (male vs. female)
- Age (younger vs. older)
- Education level
- Culture (if multi-national sample)

Sequence:
1. Configural invariance (same factor structure)
2. Metric invariance (same factor loadings)
3. Scalar invariance (same intercepts)

Goal: Achieve at least metric invariance
If scalar invariance achieved: Can compare mean scores across groups

6. WHAT DISTINGUISHES A GOOD MID-TIER TEST
Examples of Well-Validated Mid-Tier Assessments:
1. Attachment Style Measures (ECR-R)
Clear theoretical basis (attachment theory)
Continuous dimensions (anxiety √ó avoidance)
Good reliability (Œ± > 0.90)
Strong predictive validity for relationship outcomes (r = 0.40-0.60)
Why it works: Specific domain, empirically derived, measurable behaviors
2. Emotional Intelligence (MSCEIT)
Ability-based (not self-report)
Hierarchical structure
Convergent validity with performance
Discriminant from personality
Why it works: Clear construct definition, unique from Big Five
3. Growth Mindset (Dweck)
Simple continuous dimension
Predicts specific outcomes (persistence, response to failure)
Experimental evidence of malleability
Face validity
Why it works: Narrow scope, clear mechanism, actionable

What These Have in Common:
‚úì Focused scope - Not trying to measure "all of personality" ‚úì Clear mechanism - Explain HOW patterns develop and operate ‚úì Behavioral anchors - Tied to observable actions, not just feelings ‚úì Growth pathway - Show how to develop/change ‚úì Falsifiable predictions - Can be proven wrong ‚úì Incremental validity - Add something beyond Big Five

Evaluating YOUR Test Against This Standard:
Ask the developers:
1. What specifically does this measure that Big Five doesn't?
Good answer: "We focus on conflict resolution strategies, which combine trait 
tendencies with learned behaviors in relationship contexts"

Bad answer: "We measure personality more holistically" (vague, unfalsifiable)
2. What behavioral predictions does your model make?
Good answer: "People high on [dimension X] should exhibit [specific behavior Y] 
in [situation Z], which we verified in observational study"

Bad answer: "Our types represent different ways of being in the world" (too abstract)
3. How stable vs. changeable are the dimensions?
Good answer: "Core patterns show r = 0.75 at 6 months (stable trait), but 
specific strategies can change with intervention (r = 0.50)"

Bad answer: "They're both stable and changeable depending on the person" (unfalsifiable)
4. What happens when someone scores in the middle?
Good answer: "We report balanced profile; ~20% of people show this pattern"

Bad answer: "Middle scores mean you're not typed yet; take it again" (forced categorization)

7. PRACTICAL DECISION FRAMEWORK
Tier Classification for Your Test:
Based on your evaluation, classify as:
Tier A: Acceptable for Use
Criteria:
‚úì Normative scoring OR ipsative with appropriate disclaimers
‚úì Reliability Œ± > 0.70 for all scales
‚úì Test-retest > 0.70 at 1-2 months
‚úì Some validity evidence (doesn't need to be extensive)
‚úì Theoretical coherence
‚úì Appropriate humility about claims

Use cases: 
- Personal development/coaching
- Conversation starters
- Self-reflection tools
- Team communication workshops

Restrictions:
- NOT for clinical diagnosis
- NOT for employment selection
- NOT for high-stakes decisions
- Always emphasize it's one data point

Tier B: Use with Caution
Criteria:
‚ö†Ô∏è Modest reliability (0.60 < Œ± < 0.70)
‚ö†Ô∏è Limited validation data
‚ö†Ô∏è Proprietary with no independent verification
‚ö†Ô∏è Some theoretical coherence but gaps
‚ö†Ô∏è Makes modest claims

Use cases:
- Exploratory conversations only
- Icebreakers
- Hypothesis generation for further discussion

Restrictions:
- Heavy disclaimers required
- Should not drive any decisions
- Validate informally with user experience
- Consider replacing with better-validated alternative

Tier C: Do Not Use
Criteria:
‚ùå Reliability data missing or insufficient (Œ± < 0.60)
‚ùå No validity evidence
‚ùå Ipsative scores misused in statistics
‚ùå Unfalsifiable claims
‚ùå Refuses to share psychometric data
‚ùå Makes unsupported claims (predicts job success, relationship compatibility, etc.)

Recommendation: REJECT
- Ethically questionable to use
- Potentially harmful (misclassification, stereotyping)
- Legal risk if used for employment
- Better alternatives exist

8. COMPARISON QUESTIONS FOR DEVELOPERS
Direct Questions to Ask:
Scoring & Methodology:
"Is the scoring ipsative (forced-choice) or normative (Likert scale)?"
"Do you assign categorical types or continuous dimension scores?"
"What happens when someone scores near a cutpoint or boundary?"
"Can you share the complete scoring algorithm?"
Reliability: 5. "What is the internal consistency (Cronbach's alpha) for each dimension?" 6. "What is the test-retest reliability and at what interval?" 7. "What percentage of people get the same type/profile on retest?" 8. "What is your validation sample size?"
Validity: 9. "Have you conducted factor analysis? What were the fit indices?" 10. "How does your test correlate with Big Five dimensions?" 11. "What outcomes can your test predict, and what are the effect sizes?" 12. "Has any independent research validated your instrument?"
Norms: 13. "What population are your norms based on?" 14. "Do you have separate norms for different demographic groups?" 15. "How do you handle cultural/language adaptation?"
Theory: 16. "What is the theoretical basis for your dimensions/types?" 17. "Why this number of dimensions/types and not others?" 18. "What distinguishes your model from existing frameworks?" 19. "What specific behavioral predictions does your model make?"
Development: 20. "How were items developed and selected?" 21. "Have items been revised based on psychometric analysis?" 22. "Do you have a roadmap for continued validation?"

Red Flag Responses:
üö© "Our proprietary algorithm is confidential" (transparency required)
üö© "Reliability statistics don't apply to our holistic approach" (pseudoscience)
üö© "Every major company uses us" (popularity ‚â† validity)
üö© "Research is ongoing/forthcoming" (should exist before marketing)
üö© "You need our certification to interpret" (creates dependency, limits scrutiny)
üö© "Results speak for themselves" (anecdote, not data)

9. YOUR SPECIFIC EVALUATION PLAN
Phase 1: Document Review (2-4 hours)
 Request technical manual/validation report
 Review theoretical framework document
 Examine sample items
 Check for peer-reviewed publications (Google Scholar search)
 Look for independent reviews/critiques
Minimum acceptable: Technical manual with reliability data

Phase 2: Psychometric Evaluation (1-2 days with data)
If you can access raw data:
 Calculate reliability coefficients
 Run factor analysis
 Check distributions (bimodality if claiming types)
 Correlate with Big Five (if possible)
Minimum acceptable: Œ± > 0.70, test-retest > 0.70

Phase 3: Pilot Testing (2-4 weeks)
Small sample (n = 30-50):
 Administer test
 Collect qualitative feedback
 Readminister after 2-4 weeks (subset of n=20)
 Check face validity (do people recognize themselves?)
 Assess user experience (clear, engaging, actionable?)
Minimum acceptable: >70% say results are "mostly" or "very" accurate

Phase 4: Comparison (1-2 days)
Compare to alternatives:
 Cost (per administration)
 Time burden (completion time)
 Interpretability (ease of understanding)
 Actionability (does it guide development?)
 Psychometric quality (from above)
Decision matrix:
Feature                  | Your Test | Enneagram (IEQ) | Big Five (BFI-2)
------------------------|-----------|-----------------|------------------
Reliability             |    ?      |      ‚≠ê‚≠ê‚≠ê       |      ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
Validity evidence       |    ?      |      ‚≠ê‚≠ê‚≠ê       |      ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
Theoretical coherence   |    ?      |      ‚≠ê‚≠ê‚≠ê‚≠ê     |      ‚≠ê‚≠ê‚≠ê‚≠ê
User engagement         |    ?      |      ‚≠ê‚≠ê‚≠ê‚≠ê     |      ‚≠ê‚≠ê‚≠ê
Actionable insights     |    ?      |      ‚≠ê‚≠ê‚≠ê‚≠ê     |      ‚≠ê‚≠ê‚≠ê
Cost                    |    ?      |      ‚≠ê‚≠ê‚≠ê       |      ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
Focus on your domains   |    ?      |      ‚≠ê‚≠ê‚≠ê       |      ‚≠ê‚≠ê‚≠ê
Fill in your test's ratings and compare.

Phase 5: Decision (Final Recommendation)
If Tier A (Good to use):
Recommendation: ADOPT
- Use for stated purposes (growth, communication, relationships)
- Provide appropriate interpretation support
- Monitor user satisfaction
- Contribute to validation research if possible
- Revisit annually as more evidence emerges
If Tier B (Mixed quality):
Recommendation: PILOT or WAIT
- Limited pilot with feedback loops
- Use alongside more validated measures
- Heavy emphasis on "exploratory" nature
- Request developers provide missing validation data
- Set timeline for decision (e.g., revisit in 1 year)
If Tier C (Poor quality):
Recommendation: REJECT
- Too many psychometric concerns
- Better alternatives exist
- Ethical concerns about using unvalidated instrument
- Potential harm to users (misclassification, stereotyping)
- Legal/professional liability risk

10. FINAL RECOMMENDATIONS
For Your Specific Context:
Given your focus (communication, growth, relationships, outlook):
Best-Case Scenario: Your test is:
Normatively scored (Likert scales)
Has 4-7 continuous dimensions relevant to your domains
Reliability Œ± = 0.75-0.85
Modest but real validity evidence (r = 0.30-0.50 with relevant outcomes)
Clear growth pathways described
Engaging narrative interpretations
Priced reasonably
‚Üí This would be VALUABLE addition to the field
Fills gap between simple (Love Languages) and exhaustive (Big Five)
"Goldilocks zone" for coaching/development work
Proceed with adoption if psychometrics check out

Worst-Case Scenario: Your test is:
Ipsative scoring misused in correlational claims
No reliability data provided
Types with poor stability
Proprietary with no transparency
Makes grandiose claims unsupported by evidence
Essentially repackaged MBTI or Enneagram
‚Üí REJECT and recommend alternatives
Use validated Enneagram (IEQ) if narrative depth needed
Use Big Five facets if scientific rigor required
Use domain-specific validated tools (attachment styles, emotional intelligence)

Hybrid Approach (Most Realistic):
Most mid-tier tests have mixed qualities:
If it has:
‚úì Good user experience (engaging, clear, actionable)
‚úì Adequate reliability (Œ± > 0.70)
‚ö†Ô∏è Limited validity evidence
‚ö†Ô∏è Modest sample sizes
Consider:
1. Use it for LOW-STAKES applications only
   - Personal development
   - Coaching conversations
   - Team building exercises

2. ALWAYS use alongside:
   - Qualitative interview/discussion
   - Self-reflection exercises
   - Behavioral observation

3. NEVER use alone for:
   - Clinical decisions
   - Employment decisions
   - Matching/compatibility algorithms

4. FRAME appropriately:
   - "One lens among many"
   - "Starting point for exploration"
   - "Hypothesis to test through experience"

5. CONTRIBUTE to validation:
   - Collect systematic feedback
   - Share data with developers (if ethical)
   - Publish case studies or reviews

SUMMARY FOR YOUR TEAM
Key Questions to Answer:
Scoring method? (Ipsative = concern; Normative = better)
Reliability data? (Must have Œ± > 0.70)
Types or dimensions? (Dimensions = better; Types = need stability data)
What's unique? (Can't just be relabeled Big Five)
Validation sample size? (n > 200 minimum)
Independent research? (Not just creator validation)
Decision Tree:
Has reliability data (Œ± > 0.70)? 
   NO ‚Üí REJECT
   YES ‚Üí Continue
   
Is scoring normative OR appropriately handles ipsative limitations?
   NO ‚Üí REJECT  
   YES ‚Üí Continue
   
Has some validity evidence (convergent/discriminant)?
   NO ‚Üí Use with caution (Tier B)
   YES ‚Üí Continue
   
Makes realistic claims consistent with evidence?
   NO ‚Üí Use with caution (Tier B)
   YES ‚Üí Likely acceptable (Tier A)
Bottom Line: For "Enneagram-depth" personal development tools, you don't need Big Five-level perfection, but you DO need basic psychometric competence. If they can show solid reliability, some validity evidence, and theoretical coherence, it's likely acceptable for your purposes‚Äîespecially if it generates insights and engagement that more rigorous tools don't provide.
The art is balancing scientific rigor with practical utility. Just be clear about limitations and never use it for high-stakes decisions.
57-item Swipe Type assessment measures, and how we design questions, score responses, and generate reports. It‚Äôs written for (1) methodology reviewers, (2) item writers, and (3) scoring analysts. Scope (v0.1) Purpose: Surface stable relationship behavior patterns (‚ÄúSwipe Types‚Äù) from quick, mobile
pasted
our test isn't limited to just those four areas. those are examples.  our test in current state. evaluate and compare to the other four, high-level brief for now.
Scrutinized assessment's psychometric depth and interpersonal nuances.
17s










































Swipe Type Assessment: Comparative Evaluation
High-Level Psychometric Brief

1. POSITIONING vs. THE FOUR ASSESSMENTS
Complexity & Scope Placement:
Narrow ‚Üê‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Üí Comprehensive
Love Languages < SWIPE TYPE ‚âà Enneagram < MBTI < Big Five
Swipe Type occupies the "Goldilocks zone":
More than Love Languages: 12 domains vs. 5, includes conflict/boundaries/growth
Similar to Enneagram: Comparable breadth, personality + behavior fusion
Less than MBTI/Big Five: Focused on relationships/interpersonal vs. comprehensive personality
Different focus: Behavioral patterns in relationships, not general trait assessment

2. IMMEDIATE ASSESSMENT: STRONG FOUNDATION
‚úÖ Major Psychometric Advantages (Green Flags)
1. Normative Scoring (CRITICAL WIN)
Uses: 4-point Likert via swipes (Down/Left/Right/Up ‚Üí 1/2/3/4)
Result: Proper statistical analysis possible
Advantage over: Love Languages, RHETI, MBTI (all ipsative)
This is your biggest strength. You avoid the statistical nightmare that plagues Love Languages and MBTI.

2. Within-Person Normalization
Method: z_d = (mean_d - Œº_person) / œÉ_person
Purpose: Controls acquiescence bias (yea-saying)
Quality: Research-grade technique
This is sophisticated. Most tests at this tier don't do this. Controls for response style differences.

3. Continuous Scores Underneath
Outputs: POMP scores (0-100) for each domain and channel
Then: Maps to types as secondary layer
vs MBTI: Keeps continuous info, doesn't throw it away
Critical distinction: Types are interpretive layer, not the actual measurement. Much better than pure typology.

4. Co-Dominant/Mixed Type Handling
Rule: If Œî(top1, top2) ‚â§ 0.5 SD ‚Üí co-dominant
Also: Tracks all delta values, reports confidence
vs Others: MBTI forces single type even at boundaries
This shows psychometric maturity. Recognizes reality of continuous distributions.

5. Quality Flags & Confidence Metrics
Detects:
- Low variance (uniform responding)
- Excess skips (>15%)
- Speeding (<500ms median)
Results: Degrades confidence, flags in report
Essential for online assessment. Shows awareness of data quality issues.

6. Behavior-Focused Items
Example: "If I notice a task my partner dreads, I jump in without being asked"
vs Trait adjectives: "I am helpful"
Advantage: More concrete, less abstract, better criterion validity
Good practice. Behavioral items typically show better prediction of actual behavior.

‚ö†Ô∏è Yellow Flags (Proceed with Caution)
1. No Validation Data Yet (v0.1)
Status: Specification document, not validated instrument
Missing:
- Reliability coefficients (Œ±)
- Test-retest data
- Factor analysis results
- Criterion validity evidence
- Norm sample demographics
Expected at this stage, but high priority for Phase 2.

2. Small Item Counts per Domain
Current: 4-7 items per domain (12 domains, 57 total)
Concern: May not achieve Œ± ‚â• 0.80 with only 4-5 items
Target: Need 6-8 items minimum for reliability in most domains
Math check:
Expected reliability with k items (Spearman-Brown):
- 4 items, rÃÑ=0.30: Œ± = 0.63
- 6 items, rÃÑ=0.30: Œ± = 0.74  
- 8 items, rÃÑ=0.30: Œ± = 0.81

If average inter-item correlation is rÃÑ=0.40:
- 4 items: Œ± = 0.73
- 6 items: Œ± = 0.83 ‚úì
Recommendation: Pilot test to verify if 4-5 item domains hit Œ± ‚â• 0.70. May need to add 1-2 items to weaker domains.

3. 8-Type Output from Axis Mapping
Method: Two composite axes ‚Üí quadrants ‚Üí 8 types
Risk: Similar to MBTI's dichotomization problem
Mitigation: You keep continuous scores and report deltas (good!)
Concern: If users focus on type label and ignore continuous scores/deltas, you recreate MBTI issues.
Your advantage: You're explicit about co-dominance and report confidence. Keep emphasizing this.
Test this: What % of users fall near axis boundaries? If >30%, types may feel arbitrary.

4. Enneagram-Informed Personality Layer
Described as: "Light-weight indices... use as modulators"
Concern: Enneagram patterns have weak validation (as we discussed)
Uncertain: How much weight do these carry in final typing?
Question: What happens if you remove Enneagram modulation? Do types change significantly?
If yes ‚Üí Enneagram layer is load-bearing and inherits its validation problems
If no ‚Üí It's decorative/interpretive ‚Üí less concerning but potentially unnecessary
Recommendation: Be explicit about which parts are validated (channels, domains) vs. theoretical (Enneagram patterns).

5. Domain Coverage Gaps
You measure:
‚úì Relationships: Extensive (communication, time, affection, conflict, etc.)
‚úì Some personality: Identity, energy, growth
‚úó Missing: 
  - Emotional stability/neuroticism (anxiety, depression risk)
  - Conscientiousness outside relationship context
  - Cognitive style/openness beyond "identity"
This is FINE for your stated purpose (relationship patterns).
But be clear: You're not a comprehensive personality assessment. You're a relationship behavior assessment with personality moderators.

üö© Red Flags (None Critical, But Monitor)
1. Risk of Over-Interpretation
Premium report promises:
- "Communication playbook: phrases that land well / avoid"
- "Partner translation"
- Growth plans

Concern: Without validation, these could be overpromises
Recommendation: Start conservative. Don't make specific behavioral predictions until you have data showing they work.
Example of appropriate hedging: "Based on your profile, people with similar patterns often find these phrases effective..." vs "Your communication style means you should say these phrases..."

2. Channel Theory Inheritance
5 channels: Words, Time, Service, Gifts, Touch
Source: Love Languages
Problem: Love Languages has minimal validation (as we discussed)
Question: Are these actually distinct channels, or just convenient categories?
Test needed: Do your 5 channels emerge as factors when you run EFA on your item set?
Hypothesis: You might find 3-4 factors, not 5. Be prepared to adjust if data doesn't support 5-channel structure.

3. Platform-Specific Considerations
Format: Swipe-first, mobile-optimized
Concern: Do swipe directions matter? (Up/Down vs Left/Right)
         Are people consistent with what gestures mean?
Test: Does swipe direction affect responses differently than Likert buttons?
Check: Response patterns on mobile vs desktop (if you offer both)

3. COMPARISON TABLE: SWIPE TYPE vs. THE FOUR
Dimension
Love Languages
Enneagram
MBTI
Big Five
SWIPE TYPE (projected)
Domains/Factors
5
9
4 dichotomies
5 (+ 30 facets)
12 domains ‚Üí 5 channels
Items
30
108-240
93-144
44-240
57
Scoring
Ipsative
Mixed
Ipsative
Normative
Normative ‚úì
Output Format
Ranked list
Primary type
4-letter type
Continuous scores
8 types + continuous
Response Bias Control
None
None
None
Sometimes
Within-person z-score ‚úì
Mixed-Type Handling
N/A
Informal (wings)
None
N/A (continuous)
Formal delta rules ‚úì
Quality Flags
None
Rare
None
Rare
Built-in ‚úì
Reliability Target
0.70-0.85
0.65-0.87
0.87-0.92
0.86-0.92
Œ± ‚â• 0.70 (0.80 ideal)
Validation Status
Minimal
Moderate
Extensive (but problematic)
Extensive
None yet (v0.1) ‚ö†Ô∏è
Domain Focus
Relationships only
Personality broad
Cognitive preferences
Personality comprehensive
Relationships + growth
Behavioral Specificity
Low
Low-Medium
Low
Low
High ‚úì
Statistical Validity
Poor (ipsative)
Variable
Poor (dichotomy)
Excellent
Should be good (if validated)
Use Case Fit
Couples only
Personal dev
Teams/careers
Research/selection
Relationships/communication


4. TIER CLASSIFICATION (Provisional - Pending Validation)
Current Status: Pre-Tier (Not Yet Validated)
Projected Tier (if validation goes well): Tier A-
Rationale:
Strong methodological foundation (normative, normalized, quality-aware)
Appropriate scope for stated purpose
Sophisticated handling of mixed profiles
Good item development guidelines
Realistic psychometric targets
Could achieve Tier A if post-validation shows:
Domain reliability Œ± ‚â• 0.75 (most domains)
Test-retest r ‚â• 0.75 (4-week interval)
Factor structure supports 12 domains (or adjust if not)
Channel structure validates (or adjust if not)
Some criterion validity (r = 0.25-0.40 with relevant outcomes)

5. SPECIFIC CONCERNS FOR YOUR TEST
A. The "Love Languages Problem"
Issue: You're building on Love Languages foundation (5 channels), but as we discussed, LL has minimal validation.
Risk Scenario:
Your EFA might show:
- Words + Time loading together (verbal-temporal engagement)
- Service + Gifts loading together (instrumental care)
- Touch as separate factor
= 3 factors, not 5
Recommendation:
Run EFA on pilot data (n=200+) BEFORE finalizing channel structure
Be prepared to revise from 5 channels to 3-4 if data demands it
Keep 12 domains as primary measurement; channels as aggregated interpretive layer
Best case: Channels are useful interpretation heuristics even if not perfect factors
Worst case: Channels don't hold up; rely on 12 domains only

B. The "Type Paradox"
Tension:
You want: Engaging types (8 labels people can identify with)
Data shows: Continuous distributions, not discrete categories
Your solution: Types + continuous + deltas + confidence
This is GOOD, but requires user education.
Most users will:
Read their type label
Skip the continuous scores
Ignore deltas and confidence
Share type on social media ("I'm a Direct Nurturer!")
Managing this:
‚úì Your premium report emphasizes domains/channels (good)
‚úì Free summary shows "top 2 channels" not just type (good)
‚ö†Ô∏è Monitor: Do users understand they might have different type on retest if near boundary?
Test: Show users how small score changes affect type assignment. If type flips with 3-point change on one domain, that's concerning.

C. The "57-Item Challenge"
Math:
57 items / 12 domains = 4.75 items per domain average
For Œ± = 0.80, need average rÃÑ_item ‚âà 0.45 (high)

Your distribution:
Communication(7) - Good
Time(6) - Adequate  
Service(5) - Adequate
Affection(6) - Adequate
Recognition(5) - Adequate
Gifts(4) - Risky (may not hit 0.70)
Conflict(7) - Good
Boundaries(4) - Risky
Security(4) - Risky
Identity(4) - Risky
Energy(3) - Very Risky
Growth(6) - Adequate
Domains with 3-4 items will struggle to achieve Œ± ‚â• 0.70 unless items are exceptionally well-correlated.
Recommendation:
Pilot with current 57 items
Calculate Œ± for each domain
Add 2-3 items to domains with Œ± < 0.70
Consider 65-70 items for final version
Alternative: Accept Œ± = 0.65-0.70 for some domains, but report lower confidence for those areas.

D. The "Enneagram Modulation Question"
From your spec:
"Personality-pattern signals (Ennea-informed)
Light-weight indices (not formal typing):
- Centers: Head/Heart/Gut tilt
- Stances: Withdrawing/Compliant/Assertive
Use as modulators in type mapping"
Critical questions:
How much do these modulators affect type assignment?
If >20% of type variance: They're load-bearing and inherit Enneagram validation issues
If <10% of type variance: They're decorative; consider removing
Can you validate these indices?
Do your "Head/Heart/Gut" scores correlate with formal Enneagram assessments?
Or are these just theoretical overlays?
What happens without them?
Run type assignment with and without Enneagram modulation
If types change significantly: Problem (relying on unvalidated layer)
If types mostly stable: These add nuance, less critical
Recommendation: Make Enneagram layer optional/experimental in v0.1. Primary scoring uses domains and channels only. Add Enneagram modulation later if validated.

6. VALIDATION ROADMAP (What You Need)
Phase 1: Internal Structure (n=200-300)
Priority 1: Reliability
 Calculate Cronbach's Œ± for all 12 domains
 Calculate œâ (omega) if you have multi-dimensional domains
 Item-total correlations (flag items with r < 0.30)
 Inter-item correlations within domains (should be r = 0.20-0.60)
Target: Œ± ‚â• 0.70 for all domains (0.80 ideal)

Priority 2: Factor Structure
 Exploratory Factor Analysis on all 57 items
Does it yield 12 clean factors? (Or fewer?)
Do items load where you expect?
 Confirmatory Factor Analysis (if EFA looks good)
Fit indices: CFI > 0.85, RMSEA < 0.08
 Test 5-channel structure specifically
Do Words, Time, Service, Gifts, Touch items cluster as expected?
Or do you get 3-4 factors instead?
Decision point: Adjust domain/channel structure if data demands it.

Phase 2: Stability (n=100, two timepoints)
Test-Retest at 2-4 weeks:
 Domain scores (Pearson r, target ‚â• 0.75)
 Channel scores (Pearson r, target ‚â• 0.75)
 Type assignment (% agreement, target ‚â• 75%)
 Bland-Altman plots (visualize agreement)
Critical test:
For users near type boundaries (delta < 0.5 SD):
- What % get same type on retest?
- If <60%: Types are too unstable at boundaries
- If >75%: Acceptable stability

Phase 3: Construct Validity (n=200-500)
Convergent Validity: Administer alongside:
 Big Five (BFI-44, free)
 Attachment styles (ECR-R)
 Relationship satisfaction scale
 Communication skills measure
Expected patterns:
Your "Communication" domain should correlate:
- Positively with Big Five Extraversion (r ‚âà 0.40-0.60)
- Positively with relationship satisfaction (r ‚âà 0.30-0.50)

Your "Conflict" domain should correlate:
- Negatively with Big Five Neuroticism (r ‚âà -0.30 to -0.50)
- With attachment anxiety (r ‚âà 0.30-0.50)

Your channels should correlate modestly:
- With Love Languages scores (r ‚âà 0.50-0.70)
  [Higher = good convergence; but not r > 0.85 or you're redundant]
Discriminant Validity:
Should NOT correlate strongly with:
- Cognitive ability (r < 0.20)
- Unrelated personality traits (r < 0.30)

Phase 4: Criterion Validity (n=500+)
Predict relevant outcomes:
 Relationship satisfaction (current relationship)
 Communication quality (self + partner report)
 Conflict frequency and intensity
 Relationship longevity (longitudinal, if feasible)
Target effect sizes:
For Tier A acceptance:
- Single domain predicting outcome: r = 0.20-0.35
- Multiple domains combined: R¬≤ = 0.15-0.30

Example:
Communication + Conflict + Recognition domains predict 
relationship satisfaction: R¬≤ = 0.25 (acceptable)
Partner Concordance Study (powerful validation):
Both partners take assessment:
- Do complementary patterns predict satisfaction?
- Do conflict styles predict rupture frequency?
- Do channel mismatches predict unmet needs?

This would be GOLD for your use case.

Phase 5: Fairness & Generalization
Multi-Group CFA: Test measurement invariance across:
 Gender (male/female/non-binary)
 Age groups (<25, 25-40, 40+)
 Relationship status (single/dating/committed/married)
 Sexual orientation
 Race/ethnicity (if sample permits)
Goal: Achieve metric invariance (same factor loadings across groups)
If scalar invariance achieved: Can compare scores meaningfully across groups

7. RECOMMENDATIONS FOR YOUR TEAM
Immediate Actions (Pre-Launch):
1. Conservative Claims
Do say:
- "Highlights your relationship behavior patterns"
- "Starting point for self-reflection and communication"
- "Based on established relationship psychology principles"

Don't say (yet):
- "Predicts relationship success"
- "Scientifically proven"
- "Matches you with compatible partners"
Wait for validation data before making predictive/prescriptive claims.

2. Pilot Testing Protocol
Recruit n=300 diverse users:
- Mix of relationship statuses
- Age range 18-60+
- Both self-selected and recruited (avoid pure self-selection bias)

Collect:
- Full assessment responses
- Big Five (BFI-44)
- Relationship satisfaction (if applicable)
- Open-ended feedback
- Subset (n=100) retake after 2-4 weeks

3. Item Analysis & Refinement
After pilot:
1. Flag items with:
   - Very high/low endorsement (>90% or <10%)
   - Low item-total correlation (r < 0.30)
   - High cross-loading on wrong factors
   
2. Revise or replace weak items

3. Consider adding items to 3-4 item domains

4. Transparent Reporting
In reports, show:
‚úì Confidence level (high/medium/low) - you have this
‚úì Delta values for close calls - you have this
‚úì "Your scores" vs "typical range" context
‚úì Uncertainty statements: "likely," "tends to," "often"

Avoid:
‚úó Absolute statements: "You are..."
‚úó Predictions without probability: "You will..."
‚úó Diagnostic language: "Your conflict pattern indicates..."

5. Version Control & Iteration
Current: v0.1 (pre-validated)
‚Üí Pilot: v0.2 (revised items, initial reliability data)
‚Üí Beta: v0.3 (factor-analytic revision, norms)
‚Üí Launch: v1.0 (validated, stability confirmed)

Tag every result with scoring version.
Allow for algorithm updates without invalidating old results.

Medium-Term Priorities:
1. Build Norm Database
Target: n=1,000+ for initial norms
Minimum: n=500 for percentile estimates

Segment norms by:
- Age
- Relationship length
- Possibly gender (if meaningful differences)

2. Partner Dyad Studies
This is YOUR killer validation:
- Both partners take assessment
- Measure: satisfaction, conflict frequency, communication quality
- Test: Do complementary/similar patterns predict outcomes?

Sample: n=200 couples = 400 individuals
This validates your "partner translation" features.

3. Longitudinal Tracking
Follow subset of users (n=200):
- Retest at 6 months, 12 months
- Track: relationship status changes, satisfaction changes
- Test: Do growth-oriented users improve? Do types stabilize?

4. Academic Partnerships
Seek university collaborators:
- Relationship psychology labs
- Personality/assessment researchers
- Publish validation studies in peer-reviewed journals

Benefits:
- Credibility boost
- Independent validation
- Access to participant pools
- Student researchers (cost-effective)

8. COMPETITIVE POSITIONING
Your Unique Advantages:
vs Love Languages:
‚úì 12 domains vs. 5 (comprehensive)
‚úì Conflict and boundaries (major relationship predictors)
‚úì Normative scoring (statistical validity)
‚úì Quality controls (flags, confidence)
‚úì Behavior-focused items (more concrete)
vs Enneagram:
‚úì Relationship-specific (not general personality)
‚úì Clearer behavioral focus
‚úì Shorter (57 vs 144 items)
‚úì Mobile-optimized UX
‚úì More sophisticated scoring
vs MBTI:
‚úì Continuous scores maintained (not dichotomized)
‚úì Mixed-type handling (co-dominance)
‚úì Relationship focus (more relevant for your use case)
‚úì No certification paywall
‚úì Modern UX (swipe vs. dated forms)
vs Big Five:
‚úì More engaging for lay users (types, not just numbers)
‚úì Actionable relationship advice (not just trait descriptions)
‚úì Shorter (57 vs. 240 items for NEO-PI-R)
‚úì Domain-specific focus (relationships, not comprehensive)
Your market positioning:
"The relationship assessment that's actually validated"
- More than Love Languages (scope + rigor)
- Faster than Enneagram (57 vs 144 items)
- More actionable than MBTI (relationship-specific)
- More accessible than Big Five (engaging format)

9. USE CASE APPROPRIATENESS
‚úì EXCELLENT FIT FOR:
Dating app self-awareness
Help users understand their patterns
Surface potential friction points
NOT for algorithmic matching (yet - need validation first)
Couples communication workshops
Structured conversation starter
Identify complementary/conflicting patterns
Growth-oriented reframing
Relationship coaching tools
Assessment component in coaching program
Track growth over time (if retest feature)
Personalized homework/exercises
Self-development/journaling apps
Integration with reflection prompts
Longitudinal pattern tracking
Goal-setting based on domains

‚ö†Ô∏è USE WITH CAUTION:
Partner matching algorithms
WAIT for dyad validation showing compatible patterns
Don't claim "we match you with compatible types" without data
Similarity vs. complementarity is empirical question
Therapeutic/clinical settings
Not a diagnostic tool
Can supplement therapy, not replace assessment
Ensure therapist understands limitations

‚úó INAPPROPRIATE FOR:
Employment decisions (not designed for this)
Clinical diagnosis (relationship patterns ‚â† pathology)
Legal/custody decisions (not validated for high stakes)
Research (until validation completed and published)

10. FINAL VERDICT
Current Status: HIGH POTENTIAL, UNVALIDATED
Psychometric Design Quality: ‚≠ê‚≠ê‚≠ê‚≠ê (4/5)
Strong methodological foundation
Sophisticated scoring approach
Appropriate quality controls
Research-grade techniques
Validation Status: ‚≠ê (1/5)
Pre-validation specification
No reliability or validity data yet
Cannot assess actual performance
Projected Quality (if validation succeeds): ‚≠ê‚≠ê‚≠ê‚≠ê (4/5)
Would be BEST IN CLASS for relationship-focused assessment
Fills gap between pop psychology and academic tools
Potential Tier A status

Three Scenarios:
Best Case (70% probability):
Validation shows:
- Most domains hit Œ± ‚â• 0.75
- Test-retest ‚â• 0.75
- Factor structure mostly holds (maybe 10-11 domains, 4 channels)
- Modest criterion validity (r = 0.25-0.40)
- Users find it accurate and actionable

‚Üí Tier A assessment
‚Üí Best relationship-focused tool on market
‚Üí Strong commercial + research potential
Moderate Case (25% probability):
Validation shows:
- Some domains weak (Œ± = 0.60-0.70)
- Need to drop 2-3 domains or add items
- Channels don't perfectly factor (3 factors, not 5)
- Types less stable than hoped (65% agreement)
- Still useful but needs refinement

‚Üí Tier B initially, iterate to Tier A
‚Üí v2.0 with revised structure
‚Üí Still viable commercial product
Worst Case (5% probability):
Validation shows:
- Poor reliability across multiple domains
- Factor structure doesn't emerge clearly
- Types highly unstable (< 60% agreement)
- No criterion validity
- Users find results generic/inaccurate

‚Üí Tier C, needs major overhaul
‚Üí Back to drawing board on item development
‚Üí Unlikely given thoughtful specification, but possible

11. COMPARISON TO YOUR STATED GOAL
You said:
"More to it than Love Languages, much less so far at MBTI and Big Five levels, Enneagram-level depth, focused on personal assessment, communication styles, personal growth, relationship patterns"
Assessment: PERFECTLY POSITIONED ‚úì
Your test sits exactly where you intended:
‚úì More comprehensive than Love Languages (12 domains vs 5)
‚úì Similar complexity to Enneagram (multi-faceted, growth-oriented)
‚úì Less exhaustive than MBTI/Big Five (relationship-focused, not comprehensive)
‚úì Strong focus on your stated areas (communication, growth, relationships)
With one critical caveat: You need validation data to prove it works as designed.

12. CONCRETE NEXT STEPS
Week 1-2: Pre-Launch Checklist
 Finalize wording review (reading level, bias check)
 Set up data collection pipeline
 Prepare Big Five comparison instrument (BFI-44)
 Draft informed consent for validation study
 Create feedback survey for pilot users
Week 3-8: Pilot Study (n=300)
 Recruit diverse sample
 Administer Swipe Type + BFI-44
 Collect relationship satisfaction (if applicable)
 Identify subset (n=100) for retest
 Gather qualitative feedback
Week 9-12: Analysis & Refinement
 Calculate reliability for all domains
 Run EFA (exploratory factor analysis)
 Item analysis (flag weak items)
 Analyze retest data
 Revise items/domains as needed
Week 13-16: Validation Study (n=500)
 Deploy revised version (v0.2)
 Administer with validation battery
 Run CFA (confirmatory factor analysis)
 Test criterion validity
 Establish initial norms
Week 17-20: Beta Launch
 Implement v0.3 with validated structure
 Public beta with version transparency
 Monitor user feedback and data quality
 Iterate based on real-world performance
Month 6+: Research & Refinement
 Partner dyad studies
 Longitudinal tracking
 Academic collaboration
 Peer-reviewed publication
 v1.0 launch (validated)

TL;DR FOR YOUR TEAM
The Good News: Your test has excellent psychometric design - better than most at this tier. Normative scoring, within-person normalization, quality flags, and continuous scores underneath types show sophistication.
The Reality Check: You have a specification, not a validated instrument. Need n=300-500 pilot data to confirm it works as designed. This is normal and expected.
The Path Forward: Run validation studies before making strong claims. Start conservative, iterate based on data. If validation succeeds (likely given thoughtful design), you'll have best-in-class relationship assessment.
The Competitive Edge: You're positioned perfectly between pop psychology (Love Languages) and academic tools (Big Five). There's real market demand for sophisticated-but-accessible relationship assessments.
The Bottom Line: Proceed to pilot testing with confidence, but gate ambitious claims behind validation milestones.
This has strong potential to be the relationship assessment that's both engaging AND scientifically defensible - filling a real gap in the market.






